Python supporta sia la concorrenza basata sui processi che quella basata sui thread.
il problema del GIL, quando si avvia un processo con più thread, python assegna un solo interprete; asssegnando un solo interprete ad un solo processo;
tutti i thread appartenenti al processo devono utilizzare il medesimo interprete; ciò causa un rallentamento.
i thread in py vengono utilizzati per programmi di i/o boundary.
I processi invece vengono utilizzati per cpu bound.

Il problema base del multiprocessing e del multithread sta nella condivisione dei dati; difatti i dati modificabili devono essere protetti da lock
per assicurare che tutti gli accessi siano serializzati in modo che un solo thread o processo possa accedere ai dati condivisi.
Quando più thread o processi provoano ad accedere a dati condivisi, essi vengono bloccati ad eccezione di uno.
Di conseguenza è bene usare poco i lock e per il più breve tempo possibile.

La soluzione più semplice è quella di non condividere dati modificabili, ma a volte i thread hanno bisogno di modificare questi dati e la soluzione è utilizzare una struttura dati
che supporta l'accesso concorrente.
Il modolo queue fornisce diverse code thread-safe; mentre per la concorrenza basata su multiprocessing possiamo far uso dell classi multiprocessing.JoinableQueue e multiprocessing.Queue
Le code forniscono una singola sorgente di job per tutti i thread e tutti i processi, e una singola destinazione per i risultati.

Un oggetto multiprocessing.Process rappresenta un'attività che è svolta in un processo separato; i metodi della classe sono run() e start()
- run : rappresenta l'attività del processo e può essere sovrascritto; il metodo standard invoca l'oggetto collable passato come parametro al costruttore di Process
- start: dà inizio all'attività del processo e deve essere invocato al più una volta per oggetto processo
- join (timeout) : se l'argomento è none il metodo si blocca fin quando l'oggetto processo il cui join è stato invocato non termina. Se il timout è un numero positivo join si blocca al più per timout secondi
il metodo restituisce None se il processo termina o se scade il tempo di timout.

Implementazione con Futures:
andiamo a definire un insieme di futures, poi facciamo un ciclo,
in cui invochiamo cuncurrent.futures.PrecessPoolExecutor(max_workers = valore massimi lavoratori) as executor
andiamo ad assegnare ad una variabile ciò che vogliamo andare a fare attraverso executor.sumbit(function, params)
in parole povere executor.submit va a comunicare al process pool executor quali e quanti sono le funzioni da eseguire.
dopodichè aggiungiamo questa variabile all'insieme di futures dichiarati.
Fatto ciò, abbiamo a disposizione un insieme di futures; passiamo l'insieme di futures ad un'altra funzione
che si occupa di completare le funzioni (processi) precedentemente dichiarati; così va a fare un ciclo su tutti gli
elementi di concurrent.futures.as_completed(futures) e ci prendiamo il risultato del ritorno della funzione

Implementazione con Multiprocessing:
Andiamo a definire una JoinableQueue per tener traccia dei jobs, e una Queue per tenere traccia dei risultati.
Dopodichè andiamo a creare un processo per ogni valore nel range 0, maxprocess attravers
multiprocessing.Process(target = ?, args = ?), dove il target è una funzione che processerà tutte le richieste (worker)
e assegnerà a ciascune di esse una funzione, mentre args sono gli argomenti passati come parametro al worker; inoltre
andiamo a specificare che il processo è un demon e facciamo partire il processo.
Dopo aver inizializzato i processi andiamo ad aggiungere i job ai processi attraverso jobs.put((args)).
Infine andiamo a richiamare la jobs.join() che andrà ad aspettare la terminazione di tutti i processi e poi terminerà.

Il worker in particolare chiamerà la jobs.get() per prendere i parametri passati in precedenza e jobs.task_done() per
comunicare che ha terminato l'esecuzione
